{
    "_copyright": "Copyright (c) 2025 Dave Tofflemire, SigilDERG Project",
    "_license": "Licensed under the GNU Affero General Public License v3.0 (AGPLv3)",
    "_contact": "Commercial licenses available. Contact: davetmire85@gmail.com",
    "_comment": "Copy this file to config.json and customize for your setup",
    "server": {
        "name": "sigil_repos",
        "host": "127.0.0.1",
        "port": 8000,
        "log_level": "INFO",
        "allowed_hosts": [
            "*",
            "127.0.0.1",
            "localhost",
            "*.ngrok-free.app",
            "*.ngrok.io",
            "*.ngrok.app",
            "mcp.yourdomain.com"
        ]
    },
    "admin": {
        "enabled": true,
        "host": "127.0.0.1",
        "port": 8765,
        "require_api_key": true,
        "api_key": null,
        "allowed_ips": [
            "127.0.0.1",
            "::1"
        ]
    },
    "authentication": {
        "enabled": true,
        "oauth_enabled": true,
        "allow_local_bypass": true,
        "allowed_ips": [],
        "redirect_allow_list": [
            "https://chat.openai.com",
            "https://chatgpt.com",
            "https://chat.openai.com/aip/oauth/callback",
            "https://chatgpt.com/aip/oauth/callback"
        ]
    },
    "watch": {
        "_comment": "File watching for automatic index updates (requires watchdog)",
        "enabled": true,
        "debounce_seconds": 2.0,
        "ignore_dirs": [
            ".git",
            "__pycache__",
            "node_modules",
            "target",
            "build",
            "dist",
            ".ruff_cache",
            ".ruff",
            ".cache",
            ".venv",
            "venv",
            ".tox",
            ".mypy_cache",
            ".pytest_cache",
            "coverage",
            ".coverage",
            "htmlcov",
            "env",
            ".env",
            "out",
            "bin",
            "obj",
            "pkg",
            "vendor",
            "deps",
            ".gradle",
            ".idea",
            ".vscode",
            "cmake-build-debug",
            "cmake-build-release",
            "dist-newstyle",
            "_build",
            "deps/_build"
        ],
        "ignore_extensions": [
            ".pyc",
            ".pyo",
            ".pyd",
            ".so",
            ".o",
            ".a",
            ".dll",
            ".dylib",
            ".rlib",
            ".rmeta",
            ".exe",
            ".bin",
            ".class",
            ".jar",
            ".war",
            ".ear",
            ".pdf",
            ".png",
            ".jpg",
            ".jpeg",
            ".gif",
            ".svg",
            ".ico",
            ".woff",
            ".woff2",
            ".ttf",
            ".zip",
            ".tar",
            ".gz",
            ".bz2",
            ".xz",
            ".hi",
            ".beam"
        ]
    },
    "repositories": {
        "_comment": "Map of repo_name: absolute_path",
        "my-project": "/home/username/projects/my-project",
        "another-repo": "/home/username/projects/another-repo",
        "third-project": "/home/username/work/third-project"
    },
    "index": {
        "path": "~/.sigil_index",
        "skip_dirs": [
            ".git",
            "__pycache__",
            "node_modules",
            "target",
            "build",
            "dist",
            ".ruff_cache",
            ".ruff",
            ".cache",
            ".venv",
            "venv"
        ],
        "skip_extensions": [
            ".pyc",
            ".so",
            ".o",
            ".exe",
            ".bin",
            ".pdf",
            ".png",
            ".jpg",
            ".zip"
        ],
        "max_file_size_mb": 1
    },
    "admin_ui": {
        "_comment": "Autostart the Admin UI (Vite dev server) alongside the MCP server",
        "auto_start": true,
        "path": "./sigil-admin-ui",
        "command": "npm",
        "args": [
            "run",
            "dev",
            "--",
            "--host",
            "--port",
            "5173"
        ],
        "port": 5173
    },
    "external_mcp_servers": [
        {
            "name": "playwright",
            "type": "streamable-http",
            "url": "http://localhost:3001/",
            "headers": {
                "authorization": "Bearer your-token"
            },
            "init_timeout": 10,
            "tool_timeout": 120,
            "verify": true,
            "disabled": false
        },
        {
            "name": "local-stdio-example",
            "type": "stdio",
            "command": "python",
            "args": [
                "./path/to/your_mcp_server.py"
            ],
            "env": {},
            "encoding": "utf-8",
            "encoding_error_handler": "strict",
            "disabled": true
        }
    ],
    "external_mcp_auto_install": false,
    "mcp_server": {
        "_comment": "MCP transport configuration (SSE and streamable HTTP)",
        "sse_path": "/mcp/sse",
        "http_path": "/",
        "message_path": "/mcp/messages/",
        "require_token": false,
        "token": null
    },
    "embeddings": {
        "_comment": "Vector embeddings for semantic search (optional)",
        "enabled": true,
        "provider": "sentence-transformers",
        "model": "all-MiniLM-L6-v2",
        "dimension": 384,
        "cache_dir": null,
        "_providers": "sentence-transformers, openai, or llamacpp",
        "_sentence_transformers_example": {
            "provider": "sentence-transformers",
            "model": "all-MiniLM-L6-v2",
            "dimension": 384,
            "cache_dir": "./embeddings_cache"
        },
        "_openai_example": {
            "provider": "openai",
            "model": "text-embedding-3-small",
            "dimension": 1536,
            "api_key": "sk-..."
        },
        "_llamacpp_example": {
            "provider": "llamacpp",
            "model": "~/models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
            "dimension": 4096,
            "llamacpp_n_batch": 4096,
            "llamacpp_n_ubatch": 4096,
            "context_size": 2048,
            "n_gpu_layers": 0,
            "use_mlock": false
        },
        "bucket_thresholds": [
            512,
            1024,
            2048
        ],
        "target_tokens": 1024,
        "max_tokens": 2048,
        "token_overlap": 128,
        "hard_chars": 12000,
        "hard_window": 10000,
        "hard_overlap": 1000,
        "include_solution": true
    }
}
